{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Wrangling Text Data in Python\n",
    "\n",
    "Jeremy\n",
    "\n",
    "There's a world of text out there - more than you could ever want to work through. Odds are pretty good that if you have a dream for analyzing a certain type of thing there is a text corpus out there for you. It might involve a lot of work to put that corpus together, but it is within your grasp once you get working.\n",
    "\n",
    "Gathering the data is just part of the problem though. Text data is messy. Really messy. And computers don't do so well with mess. A lot of the work of working with text data is trying to clean it up and structure it in some way that the computer knows how to handle it. I'll go through a few. Here we have a text document - example.txt - that we'll load in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1\n",
      "Class Goal\n",
      "The goal of this class is to make the skills of data wrangling in Python familiar and easy to use.\n",
      "2 Class Description\n",
      "This course covers data selection, cleaning, and manipulation in Python, in- cluding reading and writing data, the Pandas library for cleaning, transforming, merging, reshaping, and data aggregation. This course will make use of Collab for handling assignments and grading. We will also use GitHub for some code management (making an account is advised but not required). This course is worth 1 credit.\n",
      "3 Readings\n",
      "There is one book for this course, Python for Data Analysis. It is published by O’Reilly Media. You can read it free online via the UVa library (the link is on the Collab page). Readings will be assigned to supplement the in class portion of the class.\n",
      "1\n",
      "4 Assessment\n",
      "Grades will be assigned based on performance on 4 homework assignments each worth 100 points. The total of all points earned will be compared with the table below to determine the final\n"
     ]
    }
   ],
   "source": [
    "filename = 'example.txt'\n",
    "with open(filename, 'r') as file_in:\n",
    "    raw_text = file_in.read()\n",
    "\n",
    "print(raw_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is the syllabus for the course. Looks pretty close to the real thing right? But our first clue that this data is unstructured is the :1000 bit at the end there. We're printing out the first 1000 characters. When we say unstructured, what we mean is that the computer has no sense of the underlying format, meaning, or structural elements of the text. It's just looking at it as a big long string, a sequence of characters thrown together. We have to do a lot of work to process the text and get it ready for working with in any program. This usually involves a few choices:\n",
    "\n",
    "* structuring\n",
    "* tagging\n",
    "\n",
    "I'll talk about the first two. Let's start with structuring. Take a look at the text given to you by default - it's a huge string. This is pretty unhelpful. You have to get the computer to think about it as discrete units. And the difficulty is that what we mean by discrete units when it comes to text is pretty complicated. There are lots of tools to help us with this. We'll be talking about nltk. \n",
    "\n",
    "Note: installing nltk requires an additional step where you download the associated corpus files that enable it to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "$ pip3 install nltk\n",
    "$ python3\n",
    ">>> import nltk\n",
    ">>> nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic structure that you want when working with text is the token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'Class',\n",
       " 'Goal',\n",
       " 'The',\n",
       " 'goal',\n",
       " 'of',\n",
       " 'this',\n",
       " 'class',\n",
       " 'is',\n",
       " 'to',\n",
       " 'make',\n",
       " 'the',\n",
       " 'skills',\n",
       " 'of',\n",
       " 'data',\n",
       " 'wrangling',\n",
       " 'in',\n",
       " 'Python',\n",
       " 'familiar',\n",
       " 'and',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'use',\n",
       " '.',\n",
       " '2',\n",
       " 'Class',\n",
       " 'Description',\n",
       " 'This',\n",
       " 'course',\n",
       " 'covers']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(raw_text)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first let's look at some baked in Python functionality. We could have imported the file differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 1\\n',\n",
       " 'Class Goal\\n',\n",
       " 'The goal of this class is to make the skills of data wrangling in Python familiar and easy to use.\\n',\n",
       " '2 Class Description\\n',\n",
       " 'This course covers data selection, cleaning, and manipulation in Python, in- cluding reading and writing data, the Pandas library for cleaning, transforming, merging, reshaping, and data aggregation. This course will make use of Collab for handling assignments and grading. We will also use GitHub for some code management (making an account is advised but not required). This course is worth 1 credit.\\n',\n",
       " '3 Readings\\n',\n",
       " 'There is one book for this course, Python for Data Analysis. It is published by O’Reilly Media. You can read it free online via the UVa library (the link is on the Collab page). Readings will be assigned to supplement the in class portion of the class.\\n',\n",
       " '1\\n',\n",
       " '4 Assessment\\n',\n",
       " 'Grades will be assigned based on performance on 4 homework assignments each worth 100 points. The total of all points earned will be compared with the table below to determine the final grade. Homework assignments will be released according to the schedule below and be due approximately one week later (there\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'example.txt'\n",
    "with open(filename, 'r') as file_in:\n",
    "    raw_text = file_in.readlines()\n",
    "\n",
    "raw_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Python's attempt to preserve the structure of the file as it was presented to us. We get, instead of one long string, a list of string. And that \\\\n character at the end of each string is Python representing the line breaks. But a \"line\" as a typographic concept is not something necessarily available to a computer. Often a paragraph that looks like several lines to us is actually input as one really long line before it gets to a paragraph break. So a single line might cross several sentences. We have a good candidate for that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This course covers data selection, cleaning, and manipulation in Python, in- cluding reading and writing data, the Pandas library for cleaning, transforming, merging, reshaping, and data aggregation. This course will make use of Collab for handling assignments and grading. We will also use GitHub for some code management (making an account is advised but not required). This course is worth 1 credit.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a page this \"line\" might actually consist of many different lines when set to a page. So we're running into lots of problems, but, in many cases, this won't matter too much. If we're working with poetry, though, we might actually care quite a bit about what a line means and about making sure that the computer's understanding of this makes sense. Far more common is to break a text down into its sentences. Note here how we are using read() and _not_ readlines. We want this paragraph as a big string to begin, and then we'll use nltk to break it down further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 1\\nClass Goal\\nThe goal of this class is to make the skills of data wrangling in Python familiar and easy to use.',\n",
       " '2 Class Description\\nThis course covers data selection, cleaning, and manipulation in Python, in- cluding reading and writing data, the Pandas library for cleaning, transforming, merging, reshaping, and data aggregation.',\n",
       " 'This course will make use of Collab for handling assignments and grading.',\n",
       " 'We will also use GitHub for some code management (making an account is advised but not required).',\n",
       " 'This course is worth 1 credit.',\n",
       " '3 Readings\\nThere is one book for this course, Python for Data Analysis.',\n",
       " 'It is published by O’Reilly Media.',\n",
       " 'You can read it free online via the UVa library (the link is on the Collab page).',\n",
       " 'Readings will be assigned to supplement the in class portion of the class.',\n",
       " '1\\n4 Assessment\\nGrades will be assigned based on performance on 4 homework assignments each worth 100 points.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "filename = 'example.txt'\n",
    "with open(filename, 'r') as file_in:\n",
    "    raw_text = file_in.read()\n",
    "\n",
    "sentences = nltk.sent_tokenize(raw_text)\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note. How. Messy. This. Is. It's looking for sentence ending punctuation, and the headers are causing problems. There is really no substitute for knowing your corpus, and there is no such thing as a one-size-fits-all solution for every text problem. You will always have to preprocess, and you will always have to massage things yourself to get the data that you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Data that isn't structured at all;\n",
    "\n",
    "Data that's too structured; - TEI\n",
    "\n",
    "\n",
    "There are lots of tools out there that can help. We'll be talking about NLTK a bit. \n",
    "\n",
    "Most basic form is text file to words, or tokens. Example\n",
    "\n",
    "But we might also be interested in the actual structure of a file. Other options.\n",
    "\n",
    "\n",
    "Breaking at paragraphs. - need to know your corpus so that you know what the markers are that you can grab onto. How can you break a longer thing into units? You need signposts.\n",
    "\n",
    "Breaking larger texts into chunks.\n",
    "\n",
    "Token vs type.\n",
    "\n",
    "Punctuation and stopwords.\n",
    "\n",
    "Filtering for each.\n",
    "\n",
    "You're preprocessing for particular analysis.\n",
    "\n",
    "\n",
    "Hand wave at the natural language processing pipeline - this is all just one part of preprocessing a text before you're ready to work with it. And it largely entails knowing what the outcomes will be. I.e. - if you're interested in knowing the parts of speech associated with a particular text, you have to actually mark a text up as such or get access to something that can. So you might add parts to the NLP pipeline accordingly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
