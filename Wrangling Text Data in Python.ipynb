{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling Text Data in Python\n",
    "\n",
    "Jeremy\n",
    "\n",
    "There's a world of text out there - more than you could ever want to work through. Odds are pretty good that if you have a dream for analyzing a certain type of thing there is a text corpus out there for you. It might involve a lot of work to put that corpus together, but it is within your grasp once you get working.\n",
    "\n",
    "Gathering the data is just part of the problem though. Text data is messy. Really messy. And computers don't do so well with mess. A lot of the work of working with text data is trying to clean it up and structure it in some way that the computer knows how to handle it. I'll go through a few. Here we have a text document - example.txt - that we'll load in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'example.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d1b8ca3ee56b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'example.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'example.txt'"
     ]
    }
   ],
   "source": [
    "filename = 'example.txt'\n",
    "with open(filename, 'r') as file_in:\n",
    "    raw_text = file_in.read()\n",
    "\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "(its all messy, nothing is structured, what does it mean to structure it)\n",
    "Throw up an example text and talk through it \n",
    "\n",
    "Data that isn't structured at all;\n",
    "\n",
    "Data that's too structured; - TEI\n",
    "\n",
    "\n",
    "Take a chunk of text - it's unhelpful as a large string. You have to get the computer to think about it as discrete units. And the difficulty is that what we mean by discrete units when it comes to text is pretty complicated.\n",
    "\n",
    "There are lots of tools out there that can help. We'll be talking about NLTK a bit. \n",
    "\n",
    "Most basic form is  text file to words, or tokens. Example\n",
    "\n",
    "But we might also be interested in the actual structure of a file. Other options.\n",
    "\n",
    "Text file to lines - useful in some cases, particularly in poetry.\n",
    "\n",
    "But a \"line\" as a typographic concept is not something necessarily available to a computer. Often a paragraph that looks like several lines to us is actually input as one really long line before it gets to a paragraph break. So a single line might cross several sentences.\n",
    "\n",
    "Breaking at paragraphs. - need to know your corpus so that you know what the markers are that you can grab onto. How can you break a longer thing into units? You need signposts.\n",
    "\n",
    "Breaking larger texts into chunks.\n",
    "\n",
    "Token vs type.\n",
    "\n",
    "Punctuation and stopwords.\n",
    "\n",
    "Filtering for each.\n",
    "\n",
    "You're preprocessing for particular analysis.\n",
    "\n",
    "\n",
    "Hand wave at the natural language processing pipeline - this is all just one part of preprocessing a text before you're ready to work with it. And it largely entails knowing what the outcomes will be. I.e. - if you're interested in knowing the parts of speech associated with a particular text, you have to actually mark a text up as such or get access to something that can. So you might add parts to the NLP pipeline accordingly. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
