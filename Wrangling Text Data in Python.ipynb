{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Wrangling Text Data in Python\n",
    "\n",
    "Processing\n",
    "\n",
    "There's a world of text out there - more than you could ever want to work through. Odds are pretty good that if you have a dream for analyzing a certain type of thing there is a text corpus out there for you. It might involve a lot of work to put that corpus together, but it is within your grasp once you get working.\n",
    "\n",
    "Gathering the data is just part of the problem though. Text data is messy. Really messy. And computers don't do so well with mess. A lot of the work of working with text data is trying to clean it up and structure it in some way that the computer knows how to handle it. I'll go through a few. Here we have a text document - example.txt - that we'll load in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1\n",
      "Class Goal\n",
      "The goal of this class is to make the skills of data wrangling in Python familiar and easy to use.\n",
      "2 Class Description\n",
      "This course covers data selection, cleaning, and manipulation in Python, in- cluding reading and writing data, the Pandas library for cleaning, transforming, merging, reshaping, and data aggregation. This course will make use of Collab for handling assignments and grading. We will also use GitHub for some code management (making an account is advised but not required). This course is worth 1 credit.\n",
      "3 Readings\n",
      "There is one book for this course, Python for Data Analysis. It is published by O’Reilly Media. You can read it free online via the UVa library (the link is on the Collab page). Readings will be assigned to supplement the in class portion of the class.\n",
      "1\n",
      "4 Assessment\n",
      "Grades will be assigned based on performance on 4 homework assignments each worth 100 points. The total of all points earned will be compared with the table below to determine the final\n"
     ]
    }
   ],
   "source": [
    "filename = 'example.txt'\n",
    "with open(filename, 'r') as file_in:\n",
    "    raw_text = file_in.read()\n",
    "\n",
    "print(raw_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is the syllabus for the course. Looks pretty close to the real thing right? But our first clue that this data is unstructured is the :1000 bit at the end there. We're printing out the first 1000 characters. When we say unstructured, what we mean is that the computer has no sense of the underlying format, meaning, or structural elements of the text. It's just looking at it as a big long string, a sequence of characters thrown together. We have to do a lot of work to process the text and get it ready for working with in any program. This usually involves a few choices:\n",
    "\n",
    "* structuring\n",
    "* tagging\n",
    "\n",
    "I'll talk about the first two. Let's start with structuring. Take a look at the text given to you by default - it's a huge string. This is pretty unhelpful. You have to get the computer to think about it as discrete units. And the difficulty is that what we mean by discrete units when it comes to text is pretty complicated. There are lots of tools to help us with this. We'll be talking about nltk. \n",
    "\n",
    "Note: installing nltk requires an additional step where you download the associated corpus files that enable it to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "$ pip3 install nltk\n",
    "$ python3\n",
    ">>> import nltk\n",
    ">>> nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first let's look at some baked in Python functionality. We could have imported the file differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 1\\n',\n",
       " 'Class Goal\\n',\n",
       " 'The goal of this class is to make the skills of data wrangling in Python familiar and easy to use.\\n',\n",
       " '2 Class Description\\n',\n",
       " 'This course covers data selection, cleaning, and manipulation in Python, in- cluding reading and writing data, the Pandas library for cleaning, transforming, merging, reshaping, and data aggregation. This course will make use of Collab for handling assignments and grading. We will also use GitHub for some code management (making an account is advised but not required). This course is worth 1 credit.\\n',\n",
       " '3 Readings\\n',\n",
       " 'There is one book for this course, Python for Data Analysis. It is published by O’Reilly Media. You can read it free online via the UVa library (the link is on the Collab page). Readings will be assigned to supplement the in class portion of the class.\\n',\n",
       " '1\\n',\n",
       " '4 Assessment\\n',\n",
       " 'Grades will be assigned based on performance on 4 homework assignments each worth 100 points. The total of all points earned will be compared with the table below to determine the final grade. Homework assignments will be released according to the schedule below and be due approximately one week later (there\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'example.txt'\n",
    "with open(filename, 'r') as file_in:\n",
    "    raw_text = file_in.readlines()\n",
    "\n",
    "raw_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Python's attempt to preserve the structure of the file as it was presented to us. We get, instead of one long string, a list of string. And that \\\\n character at the end of each string is Python representing the line breaks. But a \"line\" as a typographic concept is not something necessarily available to a computer. Often a paragraph that looks like several lines to us is actually input as one really long line before it gets to a paragraph break. So a single line might cross several sentences. We have a good candidate for that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This course covers data selection, cleaning, and manipulation in Python, in- cluding reading and writing data, the Pandas library for cleaning, transforming, merging, reshaping, and data aggregation. This course will make use of Collab for handling assignments and grading. We will also use GitHub for some code management (making an account is advised but not required). This course is worth 1 credit.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a page this \"line\" might actually consist of many different lines when set to a page. So we're running into lots of problems, but, in many cases, this won't matter too much. If we're working with poetry, though, we might actually care quite a bit about what a line means and about making sure that the computer's understanding of this makes sense. Far more common is to break a text down into its sentences. Note here how we are using read() and _not_ readlines. We want this paragraph as a big string to begin, and then we'll use nltk to break it down further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 1\\nClass Goal\\nThe goal of this class is to make the skills of data wrangling in Python familiar and easy to use.',\n",
       " '2 Class Description\\nThis course covers data selection, cleaning, and manipulation in Python, in- cluding reading and writing data, the Pandas library for cleaning, transforming, merging, reshaping, and data aggregation.',\n",
       " 'This course will make use of Collab for handling assignments and grading.',\n",
       " 'We will also use GitHub for some code management (making an account is advised but not required).',\n",
       " 'This course is worth 1 credit.',\n",
       " '3 Readings\\nThere is one book for this course, Python for Data Analysis.',\n",
       " 'It is published by O’Reilly Media.',\n",
       " 'You can read it free online via the UVa library (the link is on the Collab page).',\n",
       " 'Readings will be assigned to supplement the in class portion of the class.',\n",
       " '1\\n4 Assessment\\nGrades will be assigned based on performance on 4 homework assignments each worth 100 points.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "filename = 'example.txt'\n",
    "with open(filename, 'r') as file_in:\n",
    "    raw_text = file_in.read()\n",
    "\n",
    "sentences = nltk.sent_tokenize(raw_text)\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note. How. Messy. This. Is. It's looking for sentence ending punctuation, and the headers are causing problems. There is really no substitute for knowing your corpus, and there is no such thing as a one-size-fits-all solution for every text problem. In fact, separating things into sentences is actually pretty hard for a _human_ to do. An example brought up by Joanna Swafford illustrates problems with popular segmentation packages, but it also is pretty hard for me to tell where the sentence ends here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mrs. Rachael, I needn’t inform you who were acquainted with the late Miss Barbary’s affairs, that her means die with her and that this young lady, now her aunt is dead–”\n",
    "\n",
    "> “My aunt, sir!”\n",
    "\n",
    "> “It is really of no use carrying on a deception when no object is to be gained by it,” said Mr. Kenge smoothly, “Aunt in fact, though not in law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably three sentneces? But difficult nonetheless. You will always have to preprocess, and you will always have to massage things yourself to get the data that you want. In this case, we might use some combination, reading in lines first, then segmenting each line into sentences. Like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 1',\n",
       " 'Class Goal',\n",
       " 'The goal of this class is to make the skills of data wrangling in Python familiar and easy to use.',\n",
       " '2 Class Description',\n",
       " 'This course covers data selection, cleaning, and manipulation in Python, in- cluding reading and writing data, the Pandas library for cleaning, transforming, merging, reshaping, and data aggregation.',\n",
       " 'This course will make use of Collab for handling assignments and grading.',\n",
       " 'We will also use GitHub for some code management (making an account is advised but not required).',\n",
       " 'This course is worth 1 credit.',\n",
       " '3 Readings',\n",
       " 'There is one book for this course, Python for Data Analysis.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "filename = 'example.txt'\n",
    "with open(filename, 'r') as file_in:\n",
    "    raw_lines = file_in.readlines()\n",
    "\n",
    "sents = []\n",
    "for line in raw_lines:\n",
    "    sents.extend(nltk.sent_tokenize(line))\n",
    "\n",
    "sents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better! A lot of working with text data involves taking a set of solutions and tweaking/trading them around depending on the exact circumstances. We might also be interested in the underlying duration of the document, which case we can divide the underlying text into equal-sized chunks based on the length of characters. That's a tad more advanced, and I won't go into it. Happy to do so if you want to talk separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are way to maintain the underlying structure of a text, but at its most basic we want to work with the words themselves. These are known as tokens, and nltk gives us easy access to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1', 'Class', 'Goal', 'The', 'goal', 'of', 'this', 'class', 'is', 'to']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(raw_text)\n",
    "print(len(tokens))\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, even here the idea of a single word can be difficult for a computer to parse. This tokenizer actually takes a word hyphenated at a line break in the original and makes it into two different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in-', 'cluding']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[40:42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could write our own tokenizer to account for such edge cases, one that could handle more complicated situations like hyphenation. Or, we could loop over each element of the list and combine two elements into one when the first ends in a list.\n",
    "\n",
    "The term **token** refers to every distinct occurance of a word. We use the word **type** to refer to a unique sequence of letters, essentially a distinction between the total number of words and the total unique elements of vocabulary. We can get these by making a set of our tokens object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n",
      "222\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "unique_tokens = set(tokens)\n",
    "print(len(unique_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set returns the unique elements in a text, but we have one last piece to do here. We need to **normalize** the text, getting everything working on an even playing field so that we can actually get at the data we want. In this case, the first few tokens present the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'Class', 'Goal', 'The', 'goal']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as the computer is concerned, the words Goal and goal are two unrelated types. It is common to normalize your data to lowercase to account for such cases and actually get a sense of your working vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n",
      "222\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(set(tokens)))\n",
    "tokens = [token.lower() for token in tokens]\n",
    "print(len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gets us at the underlying data of the text. We now have a neat and organized list of lowercased tokens that we can use for further processing. This sort of preprocessing work is intimate tied, ultimately, to the kind of end result that you want to do at the end.  You might, for example, want to filter out those words that are so common that they are essentially statistical noise - words like \"the\" or \"a\". Or you might want to leave those words in for very particular reasons. This process is generally referred to, as a whole, as the natural language processing pipeline, a sequence of steps that occurs for every text and that you might need to modify depending on your needs. The shape of the pipeline will vary for your needs and largely entails knowing what the outcomes will be. I.e. - if you're interested in knowing the parts of speech associated with a particular text, you have to actually mark a text up as such or get access to something that can. So you might add parts to the NLP pipeline accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Piece on analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
